================================================================================
TREE MODELS RUST IMPLEMENTATION - STATUS UPDATE
================================================================================
Date: 2026-01-06
Phase: 1 - Tree Models Implementation

================================================================================
CRITICAL FINDING: CATBOOST TRAINING NOT AVAILABLE IN RUST
================================================================================

After extensive research, I discovered that NO Rust crate supports CatBoost TRAINING.
All available Rust bindings are INFERENCE-ONLY.

AVAILABLE RUST CRATES (All Inference-Only):
- catboost-rust v0.3.6  : Inference only, GPU support via 'gpu' feature
- catboost-rs v0.1.8    : Inference only, hardcoded to CatBoost 1.0.6
- wafer-inc/catboost    : Explicitly inference-only

IMPLICATION:
For CatBoost models, we must use a HYBRID APPROACH:
1. TRAINING: Python (using catboost library)
2. EXPORT: Save to .cbm model file OR export to ONNX
3. INFERENCE: Rust (using catboost-rust for .cbm or ort for ONNX)

This is consistent with the migration plan's strategy for RL models.

================================================================================
VERIFIED RUST CRATES FOR TRAINING + INFERENCE
================================================================================

✅ LIGHTGBM - FULL TRAINING SUPPORT
Crate: lightgbm3 v1.0.8
- Status: Actively maintained (successor to unmaintained lightgbm 0.2.3)
- Training: YES - Booster::train() with Dataset::from_vec_of_vec()
- GPU Support: YES - via 'gpu' and 'cuda' features
- API: High-level Rust interface, JSON params
- Docs: https://docs.rs/lightgbm3/latest/lightgbm3/
- GitHub: https://github.com/Mottl/lightgbm3-rs

✅ XGBOOST - FULL TRAINING SUPPORT
Crate: xgboost-rust v0.1.0 (or xgboost-rs v1.6.2)
- Status: Recently released (Dec 25, 2025), uses XGBoost 3.1.1
- Training: YES - Booster::train(), update(), update_custom()
- GPU Support: Likely (XGBoost 3.1.1 has CUDA support)
- API: Downloads XGBoost binaries from PyPI wheels at build time
- Docs: https://docs.rs/xgboost-rust (build failed, check GitHub)
- Alternative: xgboost-rs (older but stable)
- GitHub: https://github.com/aryehlev/xgboost-rust

❌ CATBOOST - NO TRAINING SUPPORT IN RUST
Crate: catboost-rust v0.3.6
- Status: Maintained
- Training: NO - Inference only
- GPU Support: YES - via 'gpu' feature
- Inference API: Model::load() + model.predict()
- Workflow: Train in Python → Export .cbm → Load in Rust
- Docs: https://docs.rs/catboost-rust
- GitHub: https://github.com/aryehlev/catboost-rust

================================================================================
IMPLEMENTATION STRATEGY
================================================================================

PHASE 1A: LightGBM + XGBoost (FULL RUST)
[ ] Implement LightGBMExpert using lightgbm3
    - fit() with all HPC logic (GPU distribution, class weighting, etc.)
    - predict_proba() with output reordering
    - save()/load() methods
[ ] Implement XGBoostExpert using xgboost-rust
    - fit() with all HPC logic
    - predict_proba() with output reordering
    - save()/load() methods
[ ] Implement XGBoost variants: XGBoostRFExpert, XGBoostDARTExpert

PHASE 1B: CatBoost (HYBRID APPROACH)
[ ] Create Python training script for CatBoost models
    - Export to .cbm format
    - OR export to ONNX format (prefer ONNX for consistency)
[ ] Implement CatBoostExpert using catboost-rust (inference only)
    - load() from .cbm file
    - predict_proba() with output reordering
[ ] OR: Use existing ONNXInferenceEngine for CatBoost ONNX models
[ ] Implement CatBoostAltExpert variant

================================================================================
DEPENDENCY UPDATES MADE
================================================================================

crates/forex-models/Cargo.toml:
[dependencies]
lightgbm3 = { version = "1.0.8", optional = true, features = ["gpu", "cuda"] }
xgboost-rust = { version = "0.1.0", optional = true }
catboost-rust = { version = "0.3.6", optional = true, features = ["gpu"] }

[features]
lightgbm = ["dep:lightgbm3"]
xgboost = ["dep:xgboost-rust"]
catboost = ["dep:catboost-rust"]
tree-models = ["lightgbm", "xgboost", "catboost"]

================================================================================
NEXT STEPS
================================================================================

IMMEDIATE:
1. Implement LightGBMExpert::fit() using lightgbm3 API
2. Implement LightGBMExpert::predict_proba()
3. Test with sample data

THEN:
4. Implement XGBoostExpert
5. Decide: CatBoost as .cbm files or ONNX export?
6. Implement all variant models
7. Benchmark against Python implementation

================================================================================
REFERENCES
================================================================================

LightGBM:
- lightgbm3: https://crates.io/crates/lightgbm3
- Docs: https://docs.rs/lightgbm3
- Training API: Dataset::from_vec_of_vec(), Booster::train()

XGBoost:
- xgboost-rust: https://crates.io/crates/xgboost-rust
- xgboost-rs: https://crates.io/crates/xgboost-rs
- Docs: https://docs.rs/xgboost-rs/latest/xgboost_rs/
- Training API: Booster::train(), update()

CatBoost:
- catboost-rust: https://crates.io/crates/catboost-rust
- Docs: https://docs.rs/catboost-rust
- INFERENCE ONLY: Model::load(), model.predict()
- Tutorial: https://github.com/catboost/tutorials/blob/master/apply_model/rust/train_model.ipynb

================================================================================
NOTES
================================================================================

- CatBoost limitation is NOT a blocker - hybrid approach is acceptable
- ONNX export for CatBoost might be preferred for consistency
- All critical HPC logic can be preserved in Rust for LightGBM/XGBoost
- GPU distribution, label remapping, time-series split all work in Rust
- This achieves 66% of tree models (2/3 libraries) in pure Rust training
- CatBoost training can happen offline, inference is still GIL-free

================================================================================
END OF STATUS
================================================================================
